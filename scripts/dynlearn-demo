#!/usr/bin/env python
#SBATCH -A MRC-BSU-SL2-CPU
#SBATCH -p bsu-cpu
#SBATCH --cpus-per-task=12
#SBATCH --job-name=NANOG
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
##SBATCH --gres=gpu:2
#SBATCH --time=14:00:00
#SBATCH --mem=29G
#SBATCH -o out/%x-%j.out
#SBATCH -e out/%x-%j.out

"""
Script to run dynlearn demo(s).
"""

from pathlib import Path
import pickle
import logging
import numpy as np
from dynlearn import demo, optimiser as opt

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

#
# Uncomment below if running inside Jupyter to automagically reload changed modules and
# fix command-line arguments
#

%reload_ext autoreload
%autoreload 2
import sys; sys.argv = [
    'dynlearn-demo',
    '-E=6',
    'active',
    '--predict-random',
]  # Used when running inside Jupyter

#
# Parse arguments
#
args = demo.arg_parser().parse_args()
# Choose a tag to distinguish these results from other results with different arguments
tag = demo.tag_from_args(args)

#
# Seed RNG
#
logger.info('Seeding RNG: {}'.format(args.seed))
np.random.seed(args.seed)

#
# Output directories
#
results_dir = Path('results')
results_dir.mkdir(exist_ok=True)
plots_dir = Path('plots')
plots_dir.mkdir(exist_ok=True)

#
# Set up (configure) simulator, loss function, GP and knots
#
sim, loss, gp, knots, knot_values, plot_args = demo.setup(args)

#
# Optimise the inputs to minimise the given loss
#
results = opt.optimise(sim, loss, gp, knots, knot_values, args)

#
# Save results
#
results_path = results_dir / f'{tag}-results.pkl'
logger.info(f'Saving results to: {results_path}')
pickle.dump(results, results_path.open('wb'))

#
# Plot history
#
history = results['history']
# TODO: this is demo specific, generalise
history[:, 0, :] /= 10  # Scale control input as it is an order of magnitude bigger than outputs
history_chart = opt.chart_history(sim, history)
history_path = plots_dir / f'{tag}-epochs.png'
logger.info(f'Saving plot to: {history_path}')
history_chart.save(str(history_path))
